{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tIntroductions  \n",
    "\n",
    "    One way to make machine learning interpretable is to use interpretable models, such as linear models or decision trees. The other option is the use of model-agnostic interpretation tools that can be applied to any supervised machine learning model. The Model-Agnostic Methods chapter deals with methods such as partial dependence plots and permutation feature importance. Model-agnostic methods work by changing the input of the machine learning model and measuring changes in the prediction output. All model-agnostic methods can be further differentiated based on whether they explain global model behavior across all data instances or individual predictions. (Chapter 1)\n",
    "\n",
    "    There is no mathematical definition of interpretability. A (non-mathematical) definition I like by Miller (2017) is: Interpretability is the degree to which a human can understand the cause of a decision. Another one is: Interpretability is the degree to which a human can consistently predict the model’s result. \n",
    "    Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017)  (Chapter 2)\n",
    "    \n",
    "    If a machine learning model performs well, why do not we just trust the model and ignore why it made a certain decision? “The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks.” (Chapter 2)\n",
    "\n",
    "    Interpretability might enable people or programs to manipulate the system. ……. The more a machine’s decision affects a person’s life, the more important it is for the machine to explain its behavior. If a machine learning model rejects a loan application, this may be completely unexpected for the applicants. (Chapter 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tResult of the interpretation method.  \n",
    "    The various interpretation methods can be roughly differentiated according to their results.  \n",
    "· Feature summary statistic  \n",
    "    Many interpretation methods provide summary statistics for each feature. Some methods return a single number per feature, such as feature importance, or a more complex result, such as the pairwise feature interaction strengths, which consist of a number for each feature pair.  \n",
    "    \n",
    "    · Feature summary visualization  \n",
    "    Most of the feature summary statistics can also be visualized. Some feature summaries are actually only meaningful if they are visualized and a table would be a wrong choice. The partial dependence of a feature is such a case. Partial dependence plots are curves that show a feature and the average predicted outcome. The best way to present partial dependences is to actually draw the curve instead of printing the coordinates.  \n",
    "\n",
    "    · Model internals (e.g. learned weights)  \n",
    "    The interpretation of intrinsically interpretable models falls into this category. Examples are the weights in linear models or the learned tree structure (the features and thresholds used for the splits) of decision trees. The lines are blurred between model internals and feature summary statistic in, for example, linear models, because the weights are both model internals and summary statistics for the features at the same time. Another method that outputs model internals is the visualization of feature detectors learned in convolutional neural networks. Interpretability methods that output model internals are by definition model-specific (see next criterion).  \n",
    "\n",
    "    · Data point  \n",
    "    This category includes all methods that return data points (already existent or newly created) to make a model interpretable. One method is called counterfactual explanations. To explain the prediction of a data instance, the method finds a similar data point by changing some of the features for which the predicted outcome changes in a relevant way (e.g. a flip in the predicted class). Another example is the identification of prototypes of predicted classes. To be useful, interpretation methods that output new data points require that the data points themselves can be interpreted. This works well for images and texts, but is less useful for tabular data with hundreds of features.  \n",
    "\n",
    "    · Intrinsically interpretable model  \n",
    "    One solution to interpreting black box models is to approximate them (either globally or locally) with an interpretable model. The interpretable model itself is interpreted by looking at internal model parameters or feature summary statistics.  \n",
    "(Chapter 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tScope of Interpretability  \n",
    "a. Global, Holistic Model Interpretability: How does the trained model make predictions?  \n",
    "b. Global Model Interpretability on a Modular Level: How do parts of the model affect predictions?  \n",
    "c. Local Interpretability for a Single Prediction: Why did the model make a certain prediction for an instance?  \n",
    "d. Local Interpretability for a Group of Predictions: Why did the model make specific predictions for a group of instances?  \n",
    "(Chapter 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Counterfactual (Chapter 6 section 1)\n",
    "\n",
    "    Idea  \n",
    "    In short, can we modify the input just a little, but let the prediction completely turn to another class?  \n",
    "    A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output. ... a counterfactual should be as similar as possible to the instance regarding feature values.  \n",
    "    A simple and naive approach to generating counterfactual explanations is searching by trial and error. This approach involves randomly changing feature values of the instance of interest and stopping when the desired output is predicted.  \n",
    "    First, we define a loss function that takes as input the instance of interest, a counterfactual and the desired (counterfactual) outcome. The loss measures how far the predicted outcome of the counterfactual is from the predefined outcome and how far the counterfactual is from the instance of interest. We can either optimize the loss directly with an optimization algorithm or by searching around the instance.  \n",
    "\n",
    "    Equation  \n",
    "    The approach suggested by Wachter et. al (2017) minimizes the following loss, $$L(x,x^\\prime,y^\\prime,\\lambda)=\\lambda\\cdot(\\hat{f}(x^\\prime)-y^\\prime)^2+d(x,x^\\prime)$$\n",
    "where $\\hat{f}(x^\\prime)$ is the model output, $y^\\prime$ is the desired output, $x$ is the input, $x^\\prime$ is the counterfactual of the input.\n",
    "The desired output may not be the actual class of the input like the adversarial example.\n",
    "The first item is to get the desired output, the second item is to not drastically change the input.  \n",
    "    Wachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without opening the black box: Automated decisions and the GDPR.” (2017) \n",
    "    \n",
    "    Algorithm\n",
    "<img src=\"./images/alg.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "    Notices  \n",
    "    a. The added noise is a saparated instance from the input as in the algorithm.  \n",
    "    b. The added noise has an initialization. The initialization may not be zero.  \n",
    "    c. For each instance you will usually find multiple counterfactual explanations.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
